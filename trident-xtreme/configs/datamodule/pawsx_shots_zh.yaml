defaults:
 # base trident datamodule configuration
 - trident

dataset_cfg:
  _method_: # get methods of _target_ object
    map: # dataset.map -> tokenization
      # kwargs for dataset.map
      function:
        _target_: trident.utils.hydra.partial
        _partial_: src.tasks.text_classification.processor.preprocess_fn
        column_names:
          text: sentence1
          text_pair: sentence2
      remove_columns:
        - "id"
        - "sentence1"
        - "sentence2"
  train:
    path: paws-x
    name: ${lang}
    split: validation
    _method_:
      shuffle:
        seed: ${seed}
      select:
        indices:
          _target_: builtins.range
          _args_:
            - 0
            - ${shots}
  val:
    _datasets_:
      validation_pawsx_lang:
        path: paws-x
        name: ${lang}
        split: validation
        _method_:
          shuffle:
            seed: ${seed}
          select:
            indices:
              _target_: builtins.range
              _args_:
                - 500
                - 2000
      test_pawsx_lang:
        path: paws-x
        name: ${lang}
        split: test
      validation_pawsx_zh:
        path: paws-x
        name: zh
        split: validation
      test_pawsx_zh:
        path: paws-x
        name: zh
        split: test
  test:
    _datasets_:
      validation_pawsx_lang:
        path: paws-x
        name: ${lang}
        split: validation
        _method_:
          shuffle:
            seed: ${seed}
          select:
            indices:
              _target_: builtins.range
              _args_:
                - 500
                - 2000
      test_pawsx_lang:
        path: paws-x
        name: ${lang}
        split: test
      validation_pawsx_zh:
        path: paws-x
        name: zh
        split: validation
      test_pawsx_zh:
        path: paws-x
        name: zh
        split: test

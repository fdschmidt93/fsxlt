# @package _global_

# to execute this experiment run:
# python run.py experiment=mnli

defaults:
  - override /trainer: default.yaml # choose trainer from 'configs/trainer/'
  - override /module: text_classification.yaml
  - override /datamodule: anli_shots.yaml
  - override /callbacks: null
  - override /config_callbacks: trident.yaml
  - override /logger: wandb.yaml

seed: 42
lang: aym
shots: ???

trainer:
  max_epochs: 50
  gpus: 1
  precision: 16
  enable_checkpointing: false

module:
  model:
    _target_: src.modules.modeling.auto_models.AutoModelForCLSClassification
    pretrained_model_name_or_path: "xlm-roberta-base"
    num_labels: 3
  # module_cfg:
  #   weights_from_checkpoint:
  #     ckpt_path: ???
  # WARNING scheduler deactivated
  scheduler: null
logger:
  wandb:
    project: "anli-shots-${lang}"
    name: "seed=${seed}-shots=${shots}-lr=${module.optimizer.lr}-epochs=${trainer.max_epochs}"

# callbacks:
#   model_checkpoint:
#     _target_: pytorch_lightning.callbacks.ModelCheckpoint
#     monitor: null
#     save_last: True # additionaly always save model from last epoch
#     verbose: False
#     dirpath: "checkpoints/"
#     filename: "epoch_{epoch:03d}"
#     auto_insert_metric_name: False

# callbacks:
#   model_checkpoint_on_epoch:
#     _target_: pytorch_lightning.callbacks.ModelCheckpoint
#     monitor: null # name of the logged metric which determines when model is improving
#     mode: "max" # can be "max" or "min"
#     every_n_epochs: 50 # truncated length of MNLI train / 16
#     verbose: true
#     save_top_k: -1 # -1 -> all models are saved
#     save_last: false # additionaly always save model from last epoch
#     dirpath: "checkpoints/"
#     auto_insert_metric_name: false
